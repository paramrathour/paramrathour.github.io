---
layout: page
title: Projects
---
<!-- Section -->
<section>
    <header class="major">
        <h2>2021</h2>
    </header>
    <div class="posts">
        <article>
            <a href="/Intelligent-and-Learning-Agents/Mountain Car/" class="image"><img src="/Thumbnails/mountain-car.gif" alt="Mountain Car" /></a>
            <h3>Mountain Car</h3>
            <p>CS747 Intelligent and Learning Agents</p>
            <p>A car is on a one-dimensional track, positioned between two "mountains". The goal is to drive up the mountain on the right; however, the car's engine is not strong enough to scale the mountain in a single pass.</p>
            <ul class="actions">
                <li><a href="/Intelligent-and-Learning-Agents/Mountain Car/" class="button">More</a></li>
            </ul>
        </article>
        <article>
            <a href="/Intelligent-and-Learning-Agents/Multi Armed Bandits/" class="image"><img src="/Thumbnails/multi-armed-bandit.gif" alt="Multi Armed Bandits" /></a>
            <h3>Multi Armed Bandits</h3>
            <p>CS747 Intelligent and Learning Agents</p>
            <p>In a bandit instance, each arm provides a random reward from a probability distribution specific to that arm, this distribution is not known a-priori and it may even change. The objective of the gambler is to maximize the sum of rewards earned through the arms which is same as minimising the 'regret'.</p>
            <ul class="actions">
                <li><a href="/Intelligent-and-Learning-Agents/Multi Armed Bandits/" class="button">More</a></li>
            </ul>
        </article>
        <article>
            <a href="/Intelligent-and-Learning-Agents/MDP Planning/" class="image"><img src="/Thumbnails/markov-decision-process.png" alt="MDP Planning" /></a>
            <h3>MDP Planning</h3>
            <p>CS747 Intelligent and Learning Agents</p>
            <p>In Reinforcement Learning problems, Markov Decision Processes (MDPs) describe the change in environment which changes its state in response to agent's actions. The state of the environment affects the immediate reward obtained by the agent, as well as the probabilities of future state transitions. The agent's objective (also called MDP Planning) is to select actions to maximize a expected long-term total reward.</p>
            <ul class="actions">
                <li><a href="/Intelligent-and-Learning-Agents/MDP Planning/" class="button">More</a></li>
            </ul>
        </article>
        <article>
            <a href="/Moustique-Cipher/" class="image"><img src="/Thumbnails/stream-cipher.png" alt="Moustique Cipher" /></a>
            <h3>Moustique Cipher</h3>
            <p>EE720 An Introduction to Number Theory and Cryptography</p>
            <p>Team Members: Param Rathour and Prathamesh Dhake</p>
            <p>Moustique is a Self-synchronizing stream cipher that was broken in the final round of the eStream project. Here we present its reduced size analog.</p>
            <ul class="actions">
                <li><a href="/Moustique-Cipher/" class="button">More</a></li>
            </ul>
        </article>
        <article>
            <a href="https://paramrathour.github.io/Microprocessors-Lab/EE337/Tennis/" class="image"><img src="/Thumbnails/tennis-scordboard-simulator.png" alt="Tennis Scordboard Simulator" /></a>
            <h3>Tennis Scordboard Simulator</h3>
            <p>EE337 Microprocessors Laboratory</p>
            <p>Simulate a scoreboard for a tennis match in the best-of-three tiebreak set format in Embedded C. The inputs of the scorer will be obtained using key presses on a keyboard connected to Pt-51 using UART.</p>
            <ul class="actions">
                <li><a href="https://paramrathour.github.io/Microprocessors-Lab/EE337/Tennis/" class="button">More</a></li>
            </ul>
        </article>
        <!-- <article>
            <a href="https://github.com/paramrathour/Microprocessors-Lab/tree/main/EE337/Lab 6" class="image"><img src="/Thumbnails/reaction-timer.png" alt="Tennis Scordboard Simulator" /></a>
            <h3>Reaction Timer</h3>
            <p>EE337 Microprocessors Laboratory</p>
            <p></p>
            <ul class="actions">
                <li><a href="https://github.com/paramrathour/Microprocessors-Lab/tree/main/EE337/Lab 6" class="button">More</a></li>
            </ul>
        </article> -->
        <article>
            <a href="https://paramrathour.github.io/Digital-Circuits-Lab/Week 5/Music Synthesizer/" class="image"><img src="/Thumbnails/music-synthesizer.png" alt="Music Synthesizer" /></a>
            <h3>Music Synthesizer</h3>
            <p>EE214 Digital Circuits Lab</p>
            <p>A music synthesizer circuit to automate the sequence of 8 musical notes to generate music using a finite state machine in VHDL.</p>
            <ul class="actions">
                <li><a href="https://paramrathour.github.io/Digital-Circuits-Lab/Week 5/Music Synthesizer/" class="button">More</a></li>
            </ul>
        </article>
    </div>
</section>
<section>
    <header class="major">
        <h2>2020</h2>
    </header>
    <div class="posts">
        <article>
            <a href="Distributed Deep Learning" class="image"><img src="/Thumbnails/distributed-deep-learning.jpg" alt="Distributed Deep Learning" /></a>
            <h3>Distributed Deep Learning</h3>
            <p>Institute Technical Summer Project <a href="https://www.tech-iitb.org/">Institute Technical Council, IIT Bombay</a></p>
            <p>Team Members: Param Rathour, Sumit Jain, Aditya Bhaskar, Ritik Mandal</p>
            <p>Deep learning models are now extensively used in various domains ranging from medical imaging to the automobile industry. Yet, to date, deep learning models fail when it comes to playing around with high-resolution data. Our model uses a hierarchy-based model to achieve synchronous parallel computations distributed to multiple machines.</p>
            <ul class="actions">
                <li><a href="Distributed Deep Learning" class="button">More</a></li>
            </ul>
        </article>
        <article>
            <a href="https://paramrathour.github.io/Team-Rocket/" class="image"><img src="https://raw.githubusercontent.com/paramrathour/Team-Rocket/main/Images/ALU Schematic.png" alt="Arithmetic Logic Unit" /></a>
            <h3>Arithmetic Logic Unit</h3>
            <p>EE224 Digital Systems</p>
            <p>Team Members: Param Rathour,  Darin Jeff, Aanal Sonara, Madhav Vadlamani</p>
            <p>A signed 16-bit ALU using Structural VHDL which computes addition, subtraction, bitwise NAND & XOR. Performs signed addition using 16-bit Koggeâ€“Stone fast adder.</p>
            <ul class="actions">
                <li><a href="https://paramrathour.github.io/Team-Rocket/" class="button">More</a></li>
            </ul>
        </article>
    </div>
</section>
<section>
    <header class="major">
        <h2>2019</h2>
    </header>
    <div class="posts">
        <article>
            <a class="image"><img src="/Thumbnails/rc-plane.png" alt="" /></a>
            <h3>Remote Control Plane</h3>
            <p>RC Plane Competition <a href="https://www.tech-iitb.org/aeroclub/">Aeromodelling Club, IIT Bombay</a></p>
            <p>Team Members: Param Rathour, Sumit Jain, Veda Pranav, Rajesh Dasari</p>
            <p>Made a Radio Controlled Trainer Plane powered by BLDC Motor</p>
            <!--ul class="actions">
                <li><a href="Remote Control Plane" class="button">More</a></li>
            </ul-->
        </article>
        <article>
            <a class="image"><img src="/Thumbnails/rc-bot.png" alt="" /></a>
            <h3>Remote Control Bot</h3>
            <p>XLR8 <a href="https://www.tech-iitb.org/erc/">Electronics and Robotics Club, IIT Bombay</a></p>
            <p>Team Members: Param Rathour, Vineet Gala, Abhinav Kumar</p>
            <p>Made a Bluetooth Controlled bot, using AT-tiny 2313 Microcontroller and L293D Motor Driver energised by Li-ion battery</p>
            <!--ul class="actions">
                <li><a href="Remote Control Bot" class="button">More</a></li>
            </ul-->
        </article>
    </div>
</section>